{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 632,
     "status": "ok",
     "timestamp": 1734057070149,
     "user": {
      "displayName": "Achmad Rifa'i R.",
      "userId": "02367818246972664028"
     },
     "user_tz": -420
    },
    "id": "XNPA_oTSYALx",
    "outputId": "a1e67db2-62b3-45f1-ee08-bda6168d6f2b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\achma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\achma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\achma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mengunduh dan Mengimpor Pustaka yang Diperlukan\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yOGwKclZYijr"
   },
   "source": [
    "stopwords : untuk mengunduh daftar stop words dalam berbagai bahasa termasuk bahasa Inggris, yang disediakan oleh NLTK.\n",
    "\n",
    "punkt : untuk mengunduh pustaka tokenisasi yang memungkinkan kita memecah kalimat menjadji kata-kata (dikenal sebagai tokenisasi).\n",
    "\n",
    "Dalam EDA, ini merupakan proses cleaning.\n",
    "kenapa? karena dalam IR ini bermaksud membersihkan kata.\n",
    "Dan di IR ini berfokus untuk pengolahan kata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1734057070149,
     "user": {
      "displayName": "Achmad Rifa'i R.",
      "userId": "02367818246972664028"
     },
     "user_tz": -420
    },
    "id": "WPcwgov9YRR-"
   },
   "outputs": [],
   "source": [
    "# Mengimpor modul stopwords dan word_tokenize dari NLTK\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypiMwx-WZR_6"
   },
   "source": [
    "corpus : untuk mengimpor modul stopwords .. yang berisi daftar kata-kata umum dalam berbagai bahasa yang bisa kita ..\n",
    "\n",
    "tokenize : untuk mengimpor fungsi word_tokenize yang digunakan untuk membagi kalimat menjadi kata-kata (token).\n",
    "\n",
    "mengimpor >> mendatangkan\n",
    "jadi pada tahapan ini kita memanggil kembali library yang sebelumnya sudah di instal sebelumnya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1734057070150,
     "user": {
      "displayName": "Achmad Rifa'i R.",
      "userId": "02367818246972664028"
     },
     "user_tz": -420
    },
    "id": "7A9aUKhDZQep"
   },
   "outputs": [],
   "source": [
    "# Contoh Kalimat yang Akan Difilter\n",
    "worf_quote = \"sir, I protest. I am not a merry man!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1734057070150,
     "user": {
      "displayName": "Achmad Rifa'i R.",
      "userId": "02367818246972664028"
     },
     "user_tz": -420
    },
    "id": "nFX3VfmUaQsM",
    "outputId": "42fa78b8-3a7c-4c11-a263-d9d9b67195a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sir', ',', 'I', 'protest', '.', 'I', 'am', 'not', 'a', 'merry', 'man', '!']\n"
     ]
    }
   ],
   "source": [
    "# Tokenisasi Kalimat\n",
    "words_in_quote = word_tokenize (worf_quote)\n",
    "print (words_in_quote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_hFDnGNkbO0w"
   },
   "source": [
    "1. fungsi word_tokenize adalah digunakan untuk membagi kalimat di dalam worf_quote menjadi kata-kata terpisah (tokens). Hasil tokenisasi akan mencakup tanda baca, sehingga kita bisa melihat seluruh elemen yang ada di dalam kelimat.\n",
    "2. Print tentu saya untuk menampilkan hasil tokenisasi seperti diatas\n",
    "\n",
    "note:\n",
    "pada case diatas, setiap 1 space maka terhitung kata. juga termasuk titik dan koma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1734057070150,
     "user": {
      "displayName": "Achmad Rifa'i R.",
      "userId": "02367818246972664028"
     },
     "user_tz": -420
    },
    "id": "JpOCcej3bkgO"
   },
   "outputs": [],
   "source": [
    "# Menentukan Stop Words Bahasa Inggris\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3s04f-9vcX2S"
   },
   "source": [
    "1. maksud dari codingan diatas berfungsi untuk memuat stop words untuk bahasa inggris.\n",
    "2. set(...) mengubah daftar stop words menjadi set, yang memungkinkan operasi pencari kata menjadi lebih cepat dan efisien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1734057070150,
     "user": {
      "displayName": "Achmad Rifa'i R.",
      "userId": "02367818246972664028"
     },
     "user_tz": -420
    },
    "id": "nCai3GiObncG",
    "outputId": "5bddc80a-fc1b-44eb-cee7-24e28f85507d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sir', ',', 'protest', '.', 'merry', 'man', '!']\n"
     ]
    }
   ],
   "source": [
    "# Memfilter Stop Words dari Teks\n",
    "filtered_list = [word for word in words_in_quote if word.casefold() not in stop_words]\n",
    "print(filtered_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ko9uHUA-dQMK"
   },
   "source": [
    "1. [...] adalah bentuk list comprehension. Kode ini mengulang setiap word (kata) dalam words_in_quote. Jika kata tersebut tidak termasuk dalam stop_words, maka akan dimasukkan ke dalam filtered_list.\n",
    "2. word.casefold() : merupakan metode casefold mengubah kata menjadi huruf kecil untuk memastikan pencarian yang tidak sensitif terhadap huruf besar dan kecil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1734057070150,
     "user": {
      "displayName": "Achmad Rifa'i R.",
      "userId": "02367818246972664028"
     },
     "user_tz": -420
    },
    "id": "TTMCBksqbnoU"
   },
   "outputs": [],
   "source": [
    "# Stemming\n",
    "# Mengimpor porterstemmer dari NLTK\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4som_4l3gMmp"
   },
   "source": [
    "Mengimpor porterstemmer dari modul nltk.stem. Algoritma Porter Stemmer akan mengubah kata menjadi bentuk dasarnya berdasarkan aturan tertentu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1734057070151,
     "user": {
      "displayName": "Achmad Rifa'i R.",
      "userId": "02367818246972664028"
     },
     "user_tz": -420
    },
    "id": "Q1Qgtk7Obn0W"
   },
   "outputs": [],
   "source": [
    "# Membuat objek porterstemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Tnh4WLrgdSp"
   },
   "source": [
    "Untuk membuat sebuah objek bernama stemmer yang menggunakan aturan stemming dari Porter Stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1734057070152,
     "user": {
      "displayName": "Achmad Rifa'i R.",
      "userId": "02367818246972664028"
     },
     "user_tz": -420
    },
    "id": "YwShLXDPboBA"
   },
   "outputs": [],
   "source": [
    "# Menyiapkan Kalimat untuk Proses Stemming\n",
    "string_for_stemming =\"\"\"\n",
    "The crew of the USS Discovery discovered many discoveries.\n",
    "Discovering is what explores do.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2m89clUcg5Iq"
   },
   "source": [
    "Pada variabel ini menyimpan teks yang akan diolah menggunakan stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1734057070152,
     "user": {
      "displayName": "Achmad Rifa'i R.",
      "userId": "02367818246972664028"
     },
     "user_tz": -420
    },
    "id": "0iqCusJlgjzf",
    "outputId": "a40f3e40-da99-4ab4-a5df-1f3658218412"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'crew', 'of', 'the', 'USS', 'Discovery', 'discovered', 'many', 'discoveries', '.', 'Discovering', 'is', 'what', 'explores', 'do', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenisasi Teks\n",
    "words = word_tokenize(string_for_stemming)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAJtTE1GhJNP"
   },
   "source": [
    "word = ...dst : Menggunakan word_tokenize untuk memisahkan setiap kata dalam string_for_stemming.\n",
    "print : untuk menampilkan daftar kata yanng dihasilkan dari tokenisasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1734057070152,
     "user": {
      "displayName": "Achmad Rifa'i R.",
      "userId": "02367818246972664028"
     },
     "user_tz": -420
    },
    "id": "0hEsCL3kgkAS",
    "outputId": "518e0d4d-f954-47f0-9d48-49f540694c03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'crew', 'of', 'the', 'uss', 'discoveri', 'discov', 'mani', 'discoveri', '.', 'discov', 'is', 'what', 'explor', 'do', '.']\n"
     ]
    }
   ],
   "source": [
    "# Menerapkan stemming pada setiap kata\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GmezkMKiB7x"
   },
   "source": [
    "1. [...dst] Merupakan bentuk lain dari list comprehension untuk melakukan stemming pada setiap kata dalam words. Hasil dari stemming akan dimasukkan dalam daftar stemmed_words.\n",
    "2. stemmer.stem(word) : merupakan fungsi step menerapkan algoritma Porter Stemmer pada setiap word, mengubah kata tersebut ke bentuk dasar yang dihasilkan oleh aturan stemming."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
